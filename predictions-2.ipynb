{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"import json\nwith open('../input/train.json') as file:\n    train = json.loads(file.read())\nwith open('../input/test.json') as file:\n    test = json.loads(file.read())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fcb7e4f5965bd1d2b7f3713c5fa2391af866225e"},"cell_type":"code","source":"from gensim.models import Word2Vec","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24dd92b6796018d9bb925f607000f0ba3721ec7a"},"cell_type":"code","source":"words = []\nfor row in train:\n    for x in row['ingredients']:\n        words.extend(x.split(' '))\nwords_set = set(words)\nprint(len(words_set))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c828afef97ff75cb22a2c5ebcb9204068e948b34"},"cell_type":"code","source":"cuisines = []\nfor row in train:\n    cuisines.append(row['cuisine'])\ncuisines_set = set(cuisines)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"424f07a39aae60eca7d1c3ea2d98c56bffc7dc47"},"cell_type":"code","source":"num_recipes = len(train)\nnum_cuisines = len(cuisines_set)\nnum_words = len(words_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e49d264da151b2949f7dbf09a4c4ac280b56f63"},"cell_type":"code","source":"y = np.zeros((num_recipes, num_cuisines))\nfor i in range(num_recipes):\n    for j in range(num_cuisines):\n        if train[i]['cuisine'] == list(cuisines_set)[j]:\n            y[i][j] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc7e2518414f1e3ada3539a4a61de4dbc2227614"},"cell_type":"code","source":"from scipy.sparse import lil_matrix\nx = lil_matrix((num_recipes, num_words))\ni = 0\nfor row in train:\n    for a in row['ingredients']:\n        for b in a.split(' '):\n            x[i, np.where(np.array(list(words_set)) == b)[0][0]] = 1\n    i += 1\n    if i%100 == 0:\n        print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e7b5c603c5a23b4f0fa616e75eaab80db25693fb"},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nmodel = Sequential()\nmodel.add(Dense(1800, input_dim=3590, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1800, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(20, activation='softmax'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam')\nmodel.fit(x, y, epochs=50, batch_size=32, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b777ddc497a1a1f3553bb075feb8cd70eb8b130"},"cell_type":"code","source":"sentences = []\nfor row in train:\n    new_sentence = []\n    for xx in row['ingredients']:\n        new_sentence.extend(xx.split(' '))\n    sentences.append(new_sentence)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31b5df790ce129aa9261c9142b873a9b35e9425a"},"cell_type":"code","source":"from gensim.models import Word2Vec\nmodel2 = Word2Vec(sentences, size=80)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27a999d80a8fb4c6e1c9939ce4c56589794fa1d3"},"cell_type":"code","source":"vectors = []\nfor sentence in sentences:\n    xx = np.array([(model2.wv[word]) for word in sentence if word in model2.wv.vocab])\n    if len(xx) == 0:\n        xx = np.zeros((2, 80))\n    if vectors == []:\n        vectors = np.mean(xx, axis=0)\n    else:\n        vectors = np.vstack([vectors, np.mean(xx, axis=0).reshape((1, 80))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b945014962003da090c88b43a1edafa289237e9"},"cell_type":"code","source":"y2 = np.zeros(num_recipes)\nfor i in range(num_recipes):\n    y2[i] = np.argmax(y[i, :])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ac96740adffd20c995f9d5be6b0bd52b28b331a"},"cell_type":"code","source":"from sklearn import svm\nclf = svm.SVC(C=4, gamma=0.2)\nclf.fit(vectors, y2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50c1cb59e601ad64caf212632388fbba7e69e9c1"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x, y2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05e58fe7e47685593968a098f79a15bbb3ae3c41"},"cell_type":"code","source":"num_recipes_test = len(test)\nx_test = lil_matrix((num_recipes_test, num_words))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06fe52acc1d5271b42f80b7c4932b108483d6113"},"cell_type":"code","source":"i = 0\nfor row in test:\n    for a in row['ingredients']:\n        for b in a.split(' '):\n            if b in words:\n                x_test[i, np.where(np.array(list(words_set)) == b)[0][0]] = 1\n    i += 1\n    if i%100 == 0:\n        print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7fb2ed10844aa2e83087353d7b994e0fea87701f"},"cell_type":"code","source":"sentences_test = []\nfor row in test:\n    new_sentence = []\n    for xx in row['ingredients']:\n        new_sentence.extend(xx.split(' '))\n    sentences_test.append(new_sentence)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0748c09e143217bfa216153fb8a82740f0555d23"},"cell_type":"code","source":"vectors_test = []\nfor sentence in sentences_test:\n    xx = np.array([(model2.wv[word]) for word in sentence if word in model2.wv.vocab])\n    if len(xx) == 0:\n        xx = np.zeros((2, 80))\n    if vectors_test == []:\n        vectors_test = np.mean(xx, axis=0)\n    else:\n        vectors_test = np.vstack([vectors_test, np.mean(xx, axis=0).reshape((1, 80))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e68e5505d6cbef90059afdba7bdba100799d7454"},"cell_type":"code","source":"predictions = model.predict(x_test)\npredictions_2 = np.argmax(predictions, axis=1)\npredictions_3 = [list(cuisines_set)[i] for i in predictions_2]\npredictions_4 = clf.predict(vectors_test)\npredictions_5 = [list(cuisines_set)[int(i)] for i in predictions_4]\npredictions_6 = lr.predict(x_test)\npredictions_7 = [list(cuisines_set)[int(i)] for i in predictions_6]\npredictions_8 = []\nfor i in range(num_recipes_test):\n    if predictions_5[i] == predictions_7[i]:\n        predictions_8.append(predictions_5[i])\n    else:\n        predictions_8.append(predictions_3[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"042e17e25c970a3160cf6a669442a03da121fedf"},"cell_type":"code","source":"result = [['id', 'cuisine']]\nresult.extend([[test[i]['id'], predictions_8[i]] for i in range(num_recipes_test)])\nimport csv\nwith open('submit.csv', 'w') as f:\n    writer = csv.writer(f)\n    for row in result:\n        writer.writerow(row)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c343dfdda5098a06c113f58514298fcbaacd18e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}